<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jagadish Krishnamoorthy</title>

    <meta name="author" content="Jagadish Krishnamoorthy">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jagadish Krishnamoorthy
                </p>
                <p>I'm a AI frameworks / Pytorch and AI models Engineer at <a href="https://www.amd.com/en.html">AMD</a> in San Francisco Bay Area. 
                  &ensp;Contributing to open source projects in personal and professional experience on a day to day basis.</a><br>
                  I deal with Python, C/C++, CMake, Jenkins, Dockers/container technology for development of AI workloads on GPU.
                </p>
                <p>
                  At AMD I've worked on Pytorch, ONNX RT, DeepSpeed, Apex, HF Transformer, AI models training and inference.</br>
                  Open source Contributor to Pytorch, DeepSpeed, ONNX RT and other AI frameworks.
                  <ul>
                    <li><p> Integrating AI frameworks such as Pytorch with <a href="https://github.com/ROCm">ROCm</a> AMD GPU AI SW stack.
                    </p></li>
                    <li><p> Integrating libraries such as BLAS, BLASLT, magma with Pytorch.
                    </p></li>
                    <li><p> Pytorch datatypes such as fp32, tf32, ocp fp8, mx fp8, mx fp4; scaling, scaled_mm gemm, matmul gemm kernels.
                    </p></li>
                    <li><p> Maintenance of Upstream Pytorch, release Pytorch branches, ROCm fork Pytorch branches.
                      <a href=https://github.com/ROCm/pytorch>https://github.com/ROCm/pytorch</a> .
                    </p></li>
                    <li><p> torch.cuda, distributed, c10d modules, single/multi GPUs training and inference using Pytorch DDP/DP.
                      RCCL / NCCL communications, training and inference of AI models such as GPT, Llama, Bert, Cifar and transformer based LLM models.
                    </p></li>
                    <li><p> GPU architecture, memory, streams, CUDA/HIP working knowledge, AI software stack ranging from application layer to GPU hardware.
                    </p></li>
                    <li><p> GPU Cloud: Multi node, multi GPU AI / ML workload distributed training using PyTorch, Composer, HuggingFace, Slurm / k8s usage for HPC jobs, Python Django project.<br>
                    </p></li>
                  </ul>
                  <hr> 
                </p>
                <h3>
                  My social links
                </h3>
                <p style="text-align:left">
                  <ul>
                    <li>Github Personal id:&ensp;<a href="https://github.com/jaglinux/">https://github.com/jaglinux/</a></li>
                    <li>Github AMD id:&ensp;<a href="https://github.com/jagadish-amd/">https://github.com/jagadish-amd/</a></li>
                    <li><a href="data/Jagadish-resume.pdf">CV / Resume</a></li>
                    <li><a href="https://www.linkedin.com/in/jagadish-krishnamoorthy/">Linkedin </a></li>
                    <li><a href="https://twitter.com/jagsposts">Twitter</a></li>
                    <li><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=Oz_2PHUAAAAJ">Google Scholar</a></li>
                    <li><a href="mailto:jagdish.krishna@gmail.com">Email</a></li>
                  </ul>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/jag.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/jag.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <hr>
                <h3>Hobby projects (AI and Blockchain) </h3>
                <ul>
                  <li><p> HuggingFace Spaces.<br>
                    <ul>
                    <li>LangChain, FaissDB for RAG, Streamlit, OpenAI LLM &ensp;<a href="https://huggingface.co/spaces/Jaglinux/url-loader">https://huggingface.co/spaces/Jaglinux/url-loader</a> </li>
                    <li>LangChain Agents &ensp;<a href="https://huggingface.co/spaces/Jaglinux/ExcelChatBot">https://huggingface.co/spaces/Jaglinux/ExcelChatBot</a> </li>
                    <li>Other HF Spaces projects @ <a href="https://huggingface.co/Jaglinux">https://huggingface.co/Jaglinux</a> </li>
                    <li>Github links for above projects @ <a href="https://github.com/jaglinux/huggingface-projects">https://github.com/jaglinux/huggingface-projects</a> <br>
                    <a href="https://github.com/jaglinux/langchain-projects">https://github.com/jaglinux/langchain-projects</a> </li>
                    </ul>
                  </p></li>
                </ul>
                <ul>
                  <li><p> BlockChain projects.<br>
                    <ul>
                    <li> Smart contract Languages: Solidity, Vyper <br>
                      Implemented EVM (Ethereum Virtual Machine) from scratch <a href="https://github.com/jaglinux/evm/blob/main/python/evm.py">https://github.com/jaglinux/evm/blob/main/python/evm.py</a>
                    </li>
                    <li>Participated in Eth Global Hackathon (Feb2022) by Polygon and built decentralized Twitter. <br>
                      The project won the shared pool prize by Polygon.<br>
                      Dapp live at Polygon mainnet <a href="https://detweet.surge.sh/">https://detweet.surge.sh/</a><br>
                      Smart contract and project code at <a href="https://github.com/jaglinux/de-tweet">https://github.com/jaglinux/de-tweet</a><br>
                    </li>
                    </ul>
                  </p></li>
                </ul>
                <hr>
                <h3>Blogs </h3>
                <ul>
                  <li><a href="https://dzone.com/articles/running-pytorch-on-gpus">https://dzone.com/articles/running-pytorch-on-gpus</a></li>
                  <li><a href="https://dzone.com/articles/all-about-gpu-threads-warps-and-wavefronts">https://dzone.com/articles/all-about-gpu-threads-warps-and-wavefronts</a></li>
                  <li><a href="https://jagsposts.blogspot.com/">https://jagsposts.blogspot.com/</a></li>
                </ul>
                <hr>
                <h3>Misc </h3>
                I have active interest in AI agents, LLM, transformer architecture encoder and decoder, training and inference on single and multi node GPU systems.
                Hit an <a href="mailto:jagdish.krishna@gmail.com">Email</a> for ideas and discussions ! 
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    
	        
					
