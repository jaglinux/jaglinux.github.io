<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>PyTorch GitHub Commits – Jagadish Krishnamoorthy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:20px;vertical-align:top;">
            <h2>PyTorch GitHub commits</h2>

            <h3>Author information</h3>
            <p>
              <strong>Name:</strong> Jagadish Krishnamoorthy<br>
              <strong>Emails:</strong><br>
              &ensp;- <a href="mailto:jagadish.krishnamoorthy@amd.com">jagadish.krishnamoorthy@amd.com</a> (recent)<br>
              &ensp;- <a href="mailto:jagdish.krishna@gmail.com">jagdish.krishna@gmail.com</a> (older)<br>
              <strong>Commits till:</strong> 2/10/2026
            </p>

            <h3>Commit history</h3>
            <ol>
              <li>f8454dc4e04 - [ROCm] Enable scaled group mm on gfx950 (#173737)</li>
              <li>ea123f27ee0 - CUDAScaledBlas - replace FBGEMM_GENAI with MSLK (#172988)</li>
              <li>0f2064707a3 - [ROCm] Unifying hipBLASLt architecture lists into common hook methods (#172791)</li>
              <li>65eeb563478 - fix build error</li>
              <li>9ae6009bc34 - GroupBlas - Check fnuz type only for gfx942</li>
              <li>c118e1fa5b3 - [ROCm] Enable scaled group mm on gfx950</li>
              <li>a6dc0642eb4 - [ROCm] Use HIPCachingAllocator for CK argument and workspace buffers (#172311)</li>
              <li>a7ee6e41091 - [ROCm] Add unit test to verify grouped GEMM CK opt‑in flag (#171901)</li>
              <li>188a1ee7549 - [ROCm] Make grouped GEMM CK opt‑in via env and default to fallback path (#171140)</li>
              <li>282d2eb4047 - [ROCm] Refactor ROCm CK config generation into shared helper (#171121)</li>
              <li>a69907a41e0 - [ROCm] Make grouped GEMM CK opt‑in via env and default to fallback path (#170159)</li>
              <li>62985304339 - [ROCm] inductor/fp8 test: Check for "cuda" in device type. (#170254)</li>
              <li>5058132088b - [ROCm] Enable group gemm on gfx90a (#169356)</li>
              <li>4887c46900e - [ROCm] Fix HIP document url. (#168220)</li>
              <li>f9b81e23e46 - [ROCm] Disable group gemm CK path when composable kernel (CK) is not enabled (#167403)</li>
              <li>dc00842b81b - [ROCm][CI] trigger magma build with gfx950 for ROCm7.1 (#167390)</li>
              <li>32d30d96cf2 - [ROCm][CI] unconditionally add gfx950, gfx115x to PYTORCH_ROCM_ARCH (#167299)</li>
              <li>af829c0dade - [ROCm] Skip nvfp4 tests on ROCm (#167066)</li>
              <li>c17aa0f1130 - [ROCm] Enable group gemm through CK (#166334)</li>
              <li>1fa520ea654 - [ROCm] Enable group gemm through CK (#166334)</li>
              <li>34ed7a8f0d1 - [ROCm] Skip test_blockwise_nvfp4_with_global_scale (#165968)</li>
              <li>8951df03ded - test_scaled_matmul_cuda: fix infer_scale_swizzle (#165788)</li>
              <li>7669ac94028 - [ROCm] Add scaled_mm v2 support. (#165528)</li>
              <li>c7e30ae4dd9 - MX: Remove redundant PLATFORM_SUPPORTS_MX_GEMM constant (#164320)</li>
              <li>264e7f68a09 - [ROCm] Fix mx fp8 and fp4 code after scaling refactor changes. (#163127)</li>
              <li>8bc4a467a7c - [ROCm] test_aot_inductor: Enable fp8 tests. (#163050)</li>
              <li>01c3c891c19 - [ROCm] Enable test_fixed_striding (#162787)</li>
              <li>6944d4b6397 - [ROCm] rocblas Aten GEMM overload for FP32 output from FP16/BF16 inputs (#162600)</li>
              <li>a8d6943d36c - ROCm: Enable overload tests from test_matmul_cuda (#161540)</li>
              <li>d2b8c0d431e - forward fix of #152198 (#161166)</li>
              <li>543896fcf33 - test_matmul_cuda: Refine MX test skipping (#161009)</li>
              <li>0d99b4e9e29 - ROCm: Enable tf32 testing on test_nn (#148945)</li>
              <li>6fa1b171955 - ROCm: Add trailing comma for consistency in gfx architecture list (#150250)</li>
              <li>ed9c8a5d136 - ROCm: Disable torch check for Multiplication of two Float8_e5m2 matrices (#148228)</li>
              <li>0ea5d1067bc - ROCm: Remove static specifier for allow_tf32 variable. (#147186)</li>
              <li>17e05cde0c4 - ROCm: Skip tests in elastic/utils/distributed_test (#144692)</li>
              <li>8f3eb843730 - ROCm: Enable 4 gpu tests for distributed config (#140319)</li>
              <li>674d59359d9 - [ROCm] Enable dist sharded_tensor test suites (#137724)</li>
              <li>ecf08a0f8b1 - [ROCm] Enable test_filtering_env_var (#84100)</li>
              <li>f58ba553b78 - [ROCm] Fix distributed tests failure and enable ROCm distributed CI (#92932)</li>
              <li>0a4e4de525a - [ROCm] add case for FP32MatMulPattern skip property (#84077)</li>
              <li>9efca7c0850 - [ROCm] [FakeTensorTest] Enable test_fallback_memory_prop (#85760)</li>
              <li>f5bfa4d0888 - [ROCm] Enable test_multiprocessing tests (#82356)</li>
              <li>7af3208412c - [ROCm] Enable test_ddp_profiling_torch_profiler (#82749)</li>
              <li>594652f0e49 - [ROCm]: Enable test_grad_layout_1devicemodule_1replicaperprocess (#82005)</li>
              <li>70e86b4562e - [test_shape_ops] Increase system memory requirement (#80369)</li>
              <li>2d354cdc2ac - [ROCm] Enable test_instantiator, test_type_hints (#78633)</li>
              <li>2bb4fce8b98 - [ROCm] TestGradients: Enable grad and gradgrad (#78401)</li>
              <li>3ee863cb7c0 - [ROCm] enable test_lobpcg_ortho_cuda_float64 (#78385)</li>
              <li>81586a6a5ec - ROCm: Enable test_distributed_spawn</li>
              <li>60e2ee3937d - ROCm: unskip c10 gloo tests</li>
              <li>6ca8272d46a - [Distributed tests] Add skip for odd world_size condition</li>
              <li>317b8fa7aef - ROCm: Enable TestUnaryUfuncsCUDA tests</li>
              <li>26ba7a92975 - ROCm: Enable test_masked_scatter_large_tensor</li>
              <li>da4a95c79a6 - [ROCm] Use hipCUB/rocPRIM scan algorithms for large index support (#68487)</li>
              <li>70a5113e03f - [ROCm] update Magma for 4.3 release (#65203)</li>
              <li>8bcf01631a1 - [ROCm] update magma (#62502)</li>
              <li>64d61901eb3 - [ROCm] Skip test_masked_scatter_large_tensor_cuda (#61313)</li>
              <li>95c26b28067 - [ROCm] disable test test_Conv2d_groups_nobias for ROCm (#59158)</li>
              <li>fd67088a578 - [Distributed test]Enable ddp_control_flow tests for ROCm (#57159)</li>
              <li>316804e373d - [test_c10d] Add wait in nccl high priority stream test (#54714)</li>
              <li>ec6a7cace3c - [ROCm] Fix the flaky test test_stream_event_nogil (#53850)</li>
              <li>0a549f9412e - [ROCm] Disable flaky tests on ROCm (#53192)</li>
              <li>2cf90982e9b - [TestZeroRedundancyOptimizer] Add multi gpu checker (#53564)</li>
              <li>506fdf9abfe - [ROCm] disable tests for ROCm 4.0.1 (#51510)</li>
              <li>eb0fe706802 - [distributed_test]Enable disabled ROCm tests. (#50421)</li>
              <li>7e05d07ca75 - [distributed_test_c10d]Enable disabled ROCm tests. (#50629)</li>
              <li>c115957df08 - [distributed] Provide parameter to pass GPU ID in barrier function (#49069)</li>
              <li>03abd81b8de - [ROCm] Enable skipped distributed global tests (#48023)</li>
              <li>1606899dbe9 - distributed_test: Map rank to GPU accordingly (#47898)</li>
            </ol>

            <p>
              <a href="../index.html">Back to home</a>
            </p>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
